defaults:
  - _self_
  - global_config

MODEL:
  architecture: "MSClipFacto"
  msclip_model_name: "Llama3-MS-CLIP-Base"
  msclip_ckpt: ""        # or path to local ckpt; leave empty to use HF download configured in build_model
  input_img_res: 264
  img_res: 224
  train_max_seq_len: 5
  val_max_seq_len: 5
  input_dim: 10
  patch_size: 16
  num_classes: 2
  channels: 10
  threshold: 0.5
  use_cls_fusion: false
  ds_labels: true
  use_l1c2l2a_adapter: false
  l1c2l2a_Adapter_loc: "${paths.l1c2l2a_Adapter_loc}"
  out_H: 25
  out_W: 25
  temp_enc_type: "attention" 
  temp_depth: 2
  use_conv_decoder: true
  freeze_msclip: true
  clearclip:
    enabled: false          
    apply_to_last_n: 1     
    keep_ffn: false        # should be false for ClearCLIP
    keep_residual: false   # should be false for ClearCLIP
    use_self_self_attn: true    
    ss_attn_iter: 1             
    ss_attn_temp: null          
    override_forward_for_dense: true

SOLVER:
  num_epochs: 50
  num_warmup_epochs: 5
  lr_scheduler: cosine
  lr_min: 1e-6
  weight_decay: 0.01
  lr_base: 3e-4            # head LR (see groups below)
  accumulate_grad_batches: 2
  grad_clip_norm: 1.0

DATASETS:
  mode: "huggingface"
  use_msclip_norm: true
  kwargs:
    mean_file: "${paths.bands_mean}"
    std_file: "${paths.bands_std}"
    with_loc: False
    with_doy: True
    bands: [
              "B2",
              "B3",
              "B4",
              "B5",
              "B6",
              "B7",
              "B8",
              "B8A",
              "B11",
              "B12",
          ]
  train:
    data_dir: "${paths.hf_data}"
    batch_size: 24
    num_workers: 8

  eval:
    data_dir: "${paths.hf_data}"
    batch_size: 24
    num_workers: 8


CHECKPOINT:
  load_from_checkpoint:
  experiment_name: "CLIP_NORM_MSCLIP_FusedDOY_ALL_DS"
  save_path: "./results/models"
  train_metrics_steps: 200 
  save_steps: 10000
  wandb_project: "${wandb.project}"
  wandb_user: "${wandb.user}"

SET-UP:
  seed: 42
  local_device_ids: [0]
