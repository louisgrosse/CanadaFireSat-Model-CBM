defaults:
  - _self_
  - global_config

MODEL:
  architecture: "L1C2L2AAdapterModel"
  msclip_model_name: "Llama3-MS-CLIP-Base"
  msclip_ckpt: ""
  channels: 10
  input_img_res: 264
  img_res: 224
  train_max_seq_len: 5
  val_max_seq_len: 5
  input_dim: 10
  patch_size: 16
  num_classes: 2
  threshold: 0.5
  use_cls_fusion: false
  ds_labels: false
  use_l1c2l2a_adapter: false
  out_H: 25
  out_W: 25
  freeze_msclip: true
  l1c2l2a_dropout: 0.1

SOLVER:
  num_epochs: 10
  loss_function: masked_dice_loss
  num_warmup_epochs: 2
  lr_start: 1e-6
  lr_min: 1e-6
  lr_base: 3e-4
  accumulate_grad_batches: 1
  weight_decay: 0.0

DATASETS:
  mode: "worldstrat"
  kwargs:
    mean_file: "${paths.bands_mean}"
    std_file: "${paths.bands_std}"
    bands: [
              "B2",
              "B3",
              "B4",
              "B5",
              "B6",
              "B7",
              "B8",
              "B8A",
              "B11",
              "B12",
          ]
    with_loc: False
    with_doy: True
  train:
    data_dir: "/home/louis/Code/wildfire-forecast/worldstrat/data"
    batch_size: 12
    num_workers: 8
  eval:
    data_dir: "/home/louis/Code/wildfire-forecast/worldstrat/data"
    batch_size: 12
    num_workers: 8


CHECKPOINT:
  load_from_checkpoint:
  experiment_name: "L1C2L2AAdapter_train"
  save_path: "./results/models"
  save_steps: 10000
  train_metrics_steps: 200
  wandb_project: "${wandb.project}"
  wandb_user: "${wandb.user}"

SET-UP:
  seed: 42
  local_device_ids: [0]
