defaults:
  - _self_
  - global_config

LOGGING:
  sanity_batches: 2   
  per_band: true      
  percentiles: [1, 50, 99]

MODEL:
  architecture: "L1C2L2AAdapterModel"
  msclip_model_name: "Llama3-MS-CLIP-Base"
  model_type: embed_affine  # or: embed_dlr, embed_wct, linear
  pool: token
  msclip_ckpt: ""
  channels: 10
  input_img_res: 264
  img_res: 224
  train_max_seq_len: 5
  val_max_seq_len: 5
  input_dim: 10
  patch_size: 16
  num_classes: 2
  threshold: 0.5
  use_cls_fusion: false
  ds_labels: false
  use_l1c2l2a_adapter: false
  out_H: 25
  out_W: 25
  freeze_msclip: true
  l1c2l2a_dropout: 0.1

SOLVER:
  lr: 1e-3
  weight_decay: 1e-4
  amp: false
  epochs: 0
  lambda_id: 1e-3               # keep near-identity
  lambda_coral: 1e-2            # only matters if MODEL.pool == "token"
  lambda_l2: 0.0

DATASETS:
  mode: "worldstrat"
  kwargs:
    mean_file: "${paths.bands_mean}"
    std_file: "${paths.bands_std}"
    bands: [
              "B2",
              "B3",
              "B4",
              "B5",
              "B6",
              "B7",
              "B8",
              "B8A",
              "B11",
              "B12",
          ]
    with_loc: False
    with_doy: True
  train:
    data_dir: "/home/louis/Code/wildfire-forecast/worldstrat/data"
    batch_size: 32
    num_workers: 8
  eval:
    data_dir: "/home/louis/Code/wildfire-forecast/worldstrat/data"
    batch_size: 32
    num_workers: 8
    io_batch_size: 64 


CHECKPOINT:
  load_from_checkpoint:
  experiment_name: "L1C2L2AAdapter_train_embed_affine"
  save_path: "./results/models"
  save_steps: 10000
  train_metrics_steps: 200
  wandb_project: "${wandb.project}"
  wandb_user: "${wandb.user}"

SET-UP:
  seed: 42
  local_device_ids: [0]
