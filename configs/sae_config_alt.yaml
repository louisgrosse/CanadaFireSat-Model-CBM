defaults:
  - _self_
  - global_config

DATA:
  # Use your manifest paths
  train_features: "/work/eceo/grosse/saeFeatures/MSCLIP_ABMIL/train/features.npy"

datamodule:
  _target_: src.data.hf_Canada.hf_datamodule.SatDataModule
  model_config: 
    architecture: "MSClipFacto"
    msclip_model_name: "Llama3-MS-CLIP-Base"
    msclip_ckpt: ""        # or path to local ckpt; leave empty to use HF download configured in build_model
    input_img_res: 264
    img_res: 224
    train_max_seq_len: 5
    val_max_seq_len: 5
    test_max_seq_len: 5
    input_dim: 10
    patch_size: 16
    num_classes: 2
    channels: 10
    threshold: 0.5
    ds_labels: true
    use_cls_fusion: false
    use_l1c2l2a_adapter: false
    unfreeze_last_block: false
    ABMIL: false           # use ABMIL module  
    use_mixer: true    
    log_abmil: false               # turn on/off logging of ABMIL attention
    log_abmil_fold_mixer: false    
    fire_class_id: 1  
    pretrained: true
    l1c2l2a_Adapter_loc: "${paths.l1c2l2a_Adapter_loc}"
    out_H: 25
    out_W: 25
    temp_enc_type: "mixer"  #attention, mixer
    freeze_msclip: true
    clearclip:
      enabled: false          
      n_last: 1
      attn_variant: "qq"    
      keep_ffn: false        # should be false for ClearCLIP
      keep_residual: false   # should be false for ClearCLIP
    sclip:
      enabled: false
      n_last: 1
    denseclip:
      enabled: false
      class_names: ["no fire", "fire"]
      prompt_template: "A satellite image of {}."
      concat_scores: true
      tau: 0.07
  train_config:
    data_dir: "${paths.hf_data}"
    batch_size: 24
    num_workers: 8
  eval_config:
    data_dir: "${paths.hf_data}"
    batch_size: 24
    num_workers: 8
  mean_file: "${paths.bands_mean}"
  std_file: "${paths.bands_std}"
  with_loc: False
  with_doy: True
  use_msclip_norm: true
  bands: [
            "B2",
            "B3",
            "B4",
            "B5",
            "B6",
            "B7",
            "B8",
            "B8A",
            "B11",
            "B12",
        ]


model:
  net:
    _target_: src.models.msclip_factorize_model.MSClipFactorizeModel  # example path
    from_msclip: true
    pretrained: false

task_name: "train-sae"

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: /home/grosse/CanadaFireSat-Model-CBM/results/topk_o...ns/${CHECKPOINT.experiment_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch_{epoch:03d}"
    monitor: "val/r2"
    verbose: False
    save_last: True
    save_top_k: 2
    mode: "max"
    auto_insert_metric_name: True
    save_weights_only: False
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null

  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: /home/grosse/CanadaFireSat-Model-CBM/results/topk_o...ns/${CHECKPOINT.experiment_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch_{epoch:03d}"
    monitor: "val/dead_features"
    verbose: False
    save_last: False
    save_top_k: 1
    mode: "min"
    auto_insert_metric_name: True
    save_weights_only: False
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null


trainer:
  _target_: pytorch_lightning.Trainer

  default_root_dir: ${hydra:run.dir}

  min_epochs: ${min_epochs}
  max_epochs: ${max_epochs}
  accumulate_grad_batches: 1 # set to 2 for 2x batch size, etc.

  accelerator: gpu
  devices: 1

  # mixed precision for extra speed-up
  # precision: 16

  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False

trainer_sae:
  _target_: pytorch_lightning.Trainer

  num_sanity_val_steps: 0

  default_root_dir: ${paths.output_dir}

  min_epochs: ${sae_min_epochs} # prevents early stopping
  max_epochs: ${sae_max_epochs}
  gradient_clip_val: 1.

  accelerator: gpu
  devices: 1

  # mixed precision for extra speed-up
  # precision: 16

  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False

sae:
  _target_: src.models.sae.plSAE
  sae_type: "topk"  #batch_topk, topk, jump,
  loss_type: "mse_auxk_true"  #mse_reanim, mse_auxk, mse_th, mse, mse_auxk_true
  optimizer_type: "adam"  
  scheduler_type: "lr_plateau"   #lr_plateau, 
  lr: ${sae_lr}
  num_samples: 100000   #max 18549
  # resample_steps: [300000, 600000, 900000, 1200000] # Rougly 1.2M steps
  resample_every_n_epochs: 100 # Equvivalent to NO resampling
  resample_batch_size: 256
  bind_init: true
  decay_k: false
  align_start_epoch: 40
  # depth_scale_shift: 2
  #criterion_kwargs:
    #penalty: 0.1               

  criterion_kwargs:
    k_aux: 256          # typical value from the paper (they say half the size of input dim)
    penalty: 0.01   # 0.03125, 

  sae_kwargs:
    input_shape: ${input_shape}
    nb_concepts : ${nb_concepts}
    top_k: ${nb_k}
    #threshold_momentum: 0.5
    device: ${oc.select:device, "cpu"}


min_epochs: 1 
warmup_epochs: 10 
max_epochs: 100 

ds_path : ${oc.env:DATASET_PATH}
ds_path_global : '' # Add Global dataset path 1 deg will complexify the rule-based search space

# Activation Data
mode_data: ""
label_down: 20

hydra:
  run:
    dir: ./results/topk_overcomplete_runs/${task_name}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}

ALIGN:
  enabled: true
  align_loss_coeff: 0
  csv_cosLoss_path: 'concept_counts50k_yake.csv'  # Path to concept names for cosine loss
  csv_phrases_path: /work/eceo/grosse/dictionnary/     # CSV with columns: concept,count
  csv_names: ['msclip_words_filtered.csv','concept_counts10k_spacy.csv', 'concept_counts50k_yake.csv','wildfire_cbm_concepts_llm_5k.csv', 'concept_counts50k_spacy.csv'] #'concept_countsCLIPDATASET.csv'
  msclip_model_name: "Llama3-MS-CLIP-Base"    
  msclip_ckpt:      
  pretrained: true
  device: "cuda"
  text_batch_size: 512
  
# SAE Arguments
use_archetypal: 
  enabled: false  #Turn off bind init if you use this
  uniform: false

input_shape: 512
nb_concepts: 4096
nb_k: 4 # See Analysis in Scaling ...
sae_lr: 1e-6
sae_batch_size: 256
sae_min_epochs: 1
sae_max_epochs: 80
sae_ckpt_path: null

tags: ["dev"]

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: 42

CHECKPOINT:
  load_from_checkpoint:
  experiment_name: "SAE_k4_4096_mixer_lr5_BM_lr6"
  save_path: "./results/topk_overcomplete_runs"
  train_metrics_steps: 200 
  save_steps: 10000
  wandb_project: "${wandb.project}"
  wandb_user: "${wandb.user}"
  group: SAE_mixer
